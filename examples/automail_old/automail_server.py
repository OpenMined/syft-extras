from __future__ import annotations

import uuid
import json
import requests
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Optional

from loguru import logger
from pydantic import BaseModel, Field
from syft_event import SyftEvents
from syft_event.types import Request
from syft_core import Client

# Create log directory if it doesn't exist
LOGS_DIR = Path("automail_logs")
LOGS_DIR.mkdir(exist_ok=True)
MESSAGE_LOG = LOGS_DIR / "message_log.json"

# Ollama API settings
OLLAMA_API_BASE = "http://localhost:11434/api"
OLLAMA_MODEL = "llama3.2"

# Create the AutoMail event box
box = SyftEvents("automail")


class MessageRequest(BaseModel):
    """A message sent from one user to another."""
    message: str = Field(description="The message content")
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), 
                               description="When the message was sent")
    message_id: str = Field(default_factory=lambda: str(uuid.uuid4()),
                           description="Unique identifier for this message")


class MessageResponse(BaseModel):
    """Response to a message, which may be AI-generated."""
    message: str = Field(description="The response message content")
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc),
                              description="When the response was generated")
    message_id: str = Field(default_factory=lambda: str(uuid.uuid4()),
                          description="Unique identifier for this response")
    ai_generated: bool = Field(default=True, description="Whether this was generated by AI")


class ContactListRequest(BaseModel):
    """Request to get the list of available contacts."""
    pass


class ContactListResponse(BaseModel):
    """Response containing the list of available contacts."""
    contacts: List[str] = Field(description="List of available contact emails")


def check_ollama():
    """Check if Ollama is running and return the available models."""
    try:
        response = requests.get(f"{OLLAMA_API_BASE}/tags")
        if response.status_code == 200:
            available_models = [model["name"] for model in response.json().get("models", [])]
            
            # Try to find a suitable model in this order of preference
            preferred_models = ["llama3.2", "llama3", "llama3:latest", "llama3.2:latest", 
                               "llama2", "mistral", "gemma"]
            
            for model in preferred_models:
                if model in available_models:
                    return model
                
            # If none of our preferred models are available, use the first one
            if available_models:
                return available_models[0]
    except:
        pass
    return None


def generate_ai_response(message: str) -> str:
    """Generate an AI response using Ollama."""
    try:
        # Check if Ollama is available
        model = check_ollama()
        if not model:
            logger.warning("Ollama not available, using fallback response")
            return "I'm unable to generate a personalized response at this time."
            
        # Call local Ollama API
        response = requests.post(
            f"{OLLAMA_API_BASE}/chat",
            json={
                "model": model,
                "messages": [{"role": "user", "content": message}],
                "stream": False
            },
            timeout=10  # Set a timeout to prevent hanging indefinitely
        )
        
        if response.status_code == 200:
            ai_response = response.json()
            if "message" in ai_response and "content" in ai_response["message"]:
                response_text = ai_response["message"]["content"]
                logger.info(f"Generated AI response: {response_text[:30]}...")
            else:
                logger.warning("Malformed response from Ollama")
                response_text = "I couldn't generate a proper response. Could you try again?"
        else:
            logger.error(f"Error from Ollama API: {response.status_code}")
            response_text = "Sorry, there was an error generating a response."
    
    except requests.exceptions.RequestException as e:
        logger.error(f"Error connecting to Ollama: {e}")
        response_text = "Sorry, I'm having trouble connecting to my reasoning engine right now."
    except Exception as e:
        logger.error(f"Error getting AI response: {e}")
        response_text = "Sorry, I'm having technical difficulties right now."
    
    return response_text


def log_message(sender: str, recipient: str, message: str, response: Optional[str] = None, ai_generated: bool = False):
    """Log the message for monitoring and history."""
    try:
        timestamp = datetime.now().strftime("%H:%M:%S")
        message_id = str(uuid.uuid4())
        
        # Read existing log if it exists
        if MESSAGE_LOG.exists():
            with open(MESSAGE_LOG, "r") as f:
                try:
                    log_data = json.load(f)
                except json.JSONDecodeError:
                    log_data = []
        else:
            log_data = []
        
        # Add new message entry
        entry = {
            "id": message_id,
            "from": sender,
            "to": recipient,
            "message": message,
            "timestamp": timestamp,
        }
        
        if response:
            entry["response"] = response
            entry["ai_generated"] = ai_generated
        
        log_data.append(entry)
        
        # Keep only last 100 messages
        if len(log_data) > 100:
            log_data = log_data[-100:]
        
        # Write back to file
        with open(MESSAGE_LOG, "w") as f:
            json.dump(log_data, f)
        
        return message_id
    except Exception as e:
        logger.error(f"Error logging message: {e}")
        return str(uuid.uuid4())


@box.on_request("/message")
def handle_message(message_req: MessageRequest, ctx: Request) -> MessageResponse:
    """Handle an incoming message and potentially generate an AI response."""
    
    sender_email = box.client.email
    recipient_email = ctx.sender.email
    
    logger.info(f"Received message from {recipient_email}: {message_req.message[:30]}...")
    
    # Log the received message
    log_message(recipient_email, sender_email, message_req.message)
    
    # Generate an AI response using Ollama
    response_text = generate_ai_response(message_req.message)
    
    # Log the response
    log_message(sender_email, recipient_email, message_req.message, response_text, ai_generated=True)
    
    # Return the response
    return MessageResponse(
        message=response_text,
        ai_generated=True
    )


@box.on_request("/contacts")
def get_contacts(req: ContactListRequest, ctx: Request) -> ContactListResponse:
    """Return a list of available contacts."""
    # In a real implementation, this would query a directory service or contacts list
    # For now, we'll just return a hardcoded list
    return ContactListResponse(
        contacts=["alice@example.com", "bob@example.com", "charlie@example.com"]
    )


if __name__ == "__main__":
    try:
        # Check if Ollama is available
        model = check_ollama()
        if model:
            OLLAMA_MODEL = model
            logger.info(f"Using Ollama model: {OLLAMA_MODEL}")
        else:
            logger.warning("Ollama not available. AI responses will be limited.")
        
        # Run the RPC server
        print("Running AutoMail RPC server on", box.app_rpc_dir)
        box.run_forever()
    except Exception as e:
        print(f"Error: {e}")
